{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44012848-77a4-47e0-9b86-9828617b5613",
   "metadata": {},
   "source": [
    "### Bishop’s University\n",
    "\n",
    "### CS 509 – Pattern Recognition\n",
    "\n",
    "### Final project: Missing data\n",
    "\n",
    "NAME: MABLINE ANDREA\n",
    "\n",
    "STUDENT ID: 002351505\n",
    "\n",
    "GROUP NAME: FP C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52d73d-8b78-4f00-993c-ac47c74db126",
   "metadata": {},
   "source": [
    "This final project aims to help you to understand the different methods to deal with missing\n",
    "data. For that purpose, you will study two approaches. The first approach is imputation-\n",
    "based approach. The principle is to assign replacement values to missing, invalid, or\n",
    "inconsistent values. We can distinguish five methods related to imputation-based approach\n",
    "that are the following:\n",
    "\n",
    "• Imputation by a single value: default value, mean, median,\n",
    "\n",
    "• Imputation by the center of the group,\n",
    "\n",
    "• Imputation from the k nearest neighbors,\n",
    "\n",
    "• Imputation by a partial mean,\n",
    "\n",
    "• Imputation by singular value decomposition.\n",
    "\n",
    "The second approach is based on EM algorithm which principle is to complete a series of\n",
    "missing data based on the maximum likelihood of all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6208e0c-3670-4729-9488-0dc52593b6d9",
   "metadata": {},
   "source": [
    "With respect to the first approach that is the imputation-based approach, we can look into the 5 methods which is related to imputation based approach\n",
    "1.The first one being imputation by a single value: which can be a default value, mean or a median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee54e5-d0b0-4336-96d7-5aa07b7e027b",
   "metadata": {},
   "source": [
    "### Imputation by a single value: default value, mean, median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047b170-ac02-4614-8620-36b82609fc7f",
   "metadata": {},
   "source": [
    "### Using default value:\n",
    "The missing values in a dataset can be replaced by default values such as 0 or unknown, this is one of the easier ways to tackle missing value, but also keeping in mind that bias can be introduced, it can create a pattern in the data that does not represent the true underlying distirbution, replacing with default values may not be the best option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b23540-a954-4181-a0ab-c4af3d634142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b674c26-b91f-48cb-b6d9-f9935b606ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consisting of missing values:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0     4.5\n",
      "1            Phone   560.0     4.7\n",
      "2  Washing-Machine     NaN     4.2\n",
      "3      Head-phones   800.0     NaN\n",
      "4         Keyboard     NaN     4.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Creating a dataset with missing values \n",
    "data = {\n",
    "    'Product': ['Tv', 'Phone', 'Washing-Machine', 'Head-phones', 'Keyboard'],  \n",
    "    'Price': [2300, 560, np.nan, 800, np.nan],  \n",
    "    'Rating': [4.5, 4.7, 4.2, np.nan, 4.8]  \n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset consisting of missing values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8687d6a-3f92-49cf-bc54-8a12d05c93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation method using default values:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0     4.5\n",
      "1            Phone   560.0     4.7\n",
      "2  Washing-Machine     0.0     4.2\n",
      "3      Head-phones   800.0     0.0\n",
      "4         Keyboard     0.0     4.8\n"
     ]
    }
   ],
   "source": [
    "df_default_values = df.fillna({'Price':0, 'Rating':0})\n",
    "\n",
    "print('Imputation method using default values:')\n",
    "print(df_default_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637aa5a4-acc8-4431-b9cd-6bde6a0cd7dc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Using mean value:\n",
    "\n",
    "Next way is to replace the missing values using the mean value, the mean value is obtained by added all the values in that particular column and by divided the it with the total number of values in that column respectively, and this value can be used to replace all the missing values in that column.\n",
    "This process can be performed for every other column that has missing values.\n",
    "\n",
    "Cons:\n",
    "Can introduce bias and not a good choice for complex data analysis. \n",
    "\n",
    "The formula to calculate the mean: Sum of all the individual data points/Total number of data points\n",
    "\n",
    "\n",
    "\n",
    "Dataset:\n",
    "\n",
    "Consider the numbers: (14, 23, 45, 67, 33, 22, 89, 54, 80, 98)\n",
    "Let us calculate the mean:\n",
    "The total number of values we have is 6 so n = 10\n",
    "\n",
    "Let us sum all the numbers together:\n",
    "Sum of the numbers: 14 + 23 + 45 + 67 + 33 + 22 + 89 + 54 + 80 + 98 = 525\n",
    "\n",
    "The mean = Sum of all the values / Total number of values \n",
    "= 525/10 = 52.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792dfcde-0e3b-4786-ac7d-f17d2dc97164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba4ca17b-648f-4fd4-86bd-d67d11f48f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consisting of missing values:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0     4.5\n",
      "1            Phone   560.0     4.7\n",
      "2  Washing-Machine     NaN     4.2\n",
      "3      Head-phones   800.0     NaN\n",
      "4         Keyboard     NaN     4.8\n",
      "Mean Value Imputation:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0    4.50\n",
      "1            Phone   560.0    4.70\n",
      "2  Washing-Machine  1220.0    4.20\n",
      "3      Head-phones   800.0    4.55\n",
      "4         Keyboard  1220.0    4.80\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataset with missing values \n",
    "data = {\n",
    "    'Product': ['Tv', 'Phone', 'Washing-Machine', 'Head-phones', 'Keyboard'],  \n",
    "    'Price': [2300, 560, np.nan, 800, np.nan],  \n",
    "    'Rating': [4.5, 4.7, 4.2, np.nan, 4.8]  \n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset consisting of missing values:\")\n",
    "print(df)\n",
    "\n",
    "df_mean = df.copy()\n",
    "df_mean['Price'] = df_mean['Price'].fillna(df_mean['Price'].mean())\n",
    "df_mean['Rating'] = df_mean['Rating'].fillna(df_mean['Rating'].mean())\n",
    "\n",
    "print(\"Mean Value Imputation:\")\n",
    "print(df_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804459cd-f5dd-4028-a86a-a9258e70725c",
   "metadata": {},
   "source": [
    "### Using median value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaabc3-6b4c-42a3-8f20-9d1f4eef2149",
   "metadata": {},
   "source": [
    "Next way to replace the missing values is by using the median value, the median value represeets the middle value, it is a measure of central tendency and is resistant to outliers especially in the case of skewed data distirbutions.\n",
    "\n",
    "The formula to calculate the median is as follows:\n",
    "The median is the middle value of the dataset, the median can be found by arranging all the values in ascending order and then splitting the values into two equal halves.\n",
    "\n",
    "For odd number of values we can consider the middle value as the median.\n",
    "For an even number of values : the median can be determined by dividing the middle two values by 2 and the resulting answer is the median.\n",
    "\n",
    "Let us look at an example:\n",
    "1.Odd Number of Values:(12, 34, 54, 13, 16)\n",
    "First we arrange the numbers in the ascending order:\n",
    "So the resulting values will be : (12, 13, 16, 34, 54)\n",
    "\n",
    "In this case the middle value is 16 so the median here is 16.\n",
    "\n",
    "2.Even number of values:(29, 17, 11, 09, 20, 10, 25, 30)\n",
    "Firstly the values should be arranged in ascending order:\n",
    "So the values even number of values in ascending order will be : (09, 10, 11, 17, 20, 25, 29, 30)\n",
    "\n",
    "In this case the middle two values are : 17+20 = 37\n",
    "so we divide 37/2 and the median is 18.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9236268f-cf7f-494e-9662-1318b092bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consisting of missing values:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0     4.5\n",
      "1            Phone   560.0     4.7\n",
      "2  Washing-Machine     NaN     4.2\n",
      "3      Head-phones   800.0     NaN\n",
      "4         Keyboard     NaN     4.8\n",
      "Mean Value Imputation:\n",
      "           Product   Price  Rating\n",
      "0               Tv  2300.0     4.5\n",
      "1            Phone   560.0     4.7\n",
      "2  Washing-Machine   800.0     4.2\n",
      "3      Head-phones   800.0     4.6\n",
      "4         Keyboard   800.0     4.8\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataset with missing values \n",
    "data = {\n",
    "    'Product': ['Tv', 'Phone', 'Washing-Machine', 'Head-phones', 'Keyboard'],  \n",
    "    'Price': [2300, 560, np.nan, 800, np.nan],  \n",
    "    'Rating': [4.5, 4.7, 4.2, np.nan, 4.8]  \n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset consisting of missing values:\")\n",
    "print(df)\n",
    "\n",
    "df_median = df.copy()\n",
    "df_median['Price'] = df_median['Price'].fillna(df_median['Price'].median())\n",
    "df_median['Rating'] = df_median['Rating'].fillna(df_median['Rating'].median())\n",
    "\n",
    "print(\"Mean Value Imputation:\")\n",
    "print(df_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57a810-fa1c-434f-bd19-26c45b436be2",
   "metadata": {},
   "source": [
    "### • Imputation by the center of the group\n",
    "With respect to imputation by center of the group, we can replace missing values in the dataset by making use of the ean, median or probably the mode value, and we can do this by grouping each category and estimating the details within each category respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad9f096e-c778-400f-81cd-67ebe73e54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c9b931-452d-44e3-9f75-df8e3f20ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "     Stores     Product  Sales\n",
      "0    Costco  Dishwasher  310.0\n",
      "1    Costco   Table-Fan  250.0\n",
      "2  Best_Buy  Dishwasher    NaN\n",
      "3  Best_Buy   Table-Fan  390.0\n",
      "4      Maxi  Dishwasher  170.0\n",
      "5      Maxi   Table-Fan    NaN\n",
      "6   Walmart  Dishwasher  230.0\n",
      "7   Walmart   Table-Fan  175.0\n",
      "\n",
      "Mean Sales per Store:\n",
      "0    280.0\n",
      "1    280.0\n",
      "2    390.0\n",
      "3    390.0\n",
      "4    170.0\n",
      "5    170.0\n",
      "6    202.5\n",
      "7    202.5\n",
      "Name: Sales, dtype: float64\n",
      "\n",
      "Dataset After Imputation:\n",
      "     Stores     Product  Sales\n",
      "0    costco  Dishwasher  310.0\n",
      "1    costco   Table-Fan  250.0\n",
      "2  best_buy  Dishwasher  390.0\n",
      "3  best_buy   Table-Fan  390.0\n",
      "4      maxi  Dishwasher  170.0\n",
      "5      maxi   Table-Fan  170.0\n",
      "6   walmart  Dishwasher  230.0\n",
      "7   walmart   Table-Fan  175.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Stores': ['Costco', 'Costco', 'Best_Buy', 'Best_Buy', 'Maxi', 'Maxi', 'Walmart', 'Walmart'],\n",
    "    'Product': ['Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan'],\n",
    "    'Sales': [310, 250, np.nan, 390, 170, np.nan, 230, 175]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "df['Stores'] = df['Stores'].str.lower()\n",
    "\n",
    "group_mean = df.groupby('Stores')['Sales'].transform('mean')\n",
    "\n",
    "print(\"\\nMean Sales per Store:\")\n",
    "print(group_mean)\n",
    "\n",
    "\n",
    "df['Sales'] = df['Sales'].fillna(group_mean)\n",
    "\n",
    "print(\"\\nDataset After Imputation:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1627d9a6-0a25-45c2-905b-7e0f9525d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "     Stores     Product  Sales\n",
      "0    Costco  Dishwasher  310.0\n",
      "1    Costco   Table-Fan  250.0\n",
      "2  Best_Buy  Dishwasher    NaN\n",
      "3  Best_Buy   Table-Fan  390.0\n",
      "4      Maxi  Dishwasher  170.0\n",
      "5      Maxi   Table-Fan    NaN\n",
      "6   Walmart  Dishwasher  230.0\n",
      "7   Walmart   Table-Fan  175.0\n",
      "\n",
      "Mean Sales per Store:\n",
      "0    280.0\n",
      "1    280.0\n",
      "2    390.0\n",
      "3    390.0\n",
      "4    170.0\n",
      "5    170.0\n",
      "6    202.5\n",
      "7    202.5\n",
      "Name: Sales, dtype: float64\n",
      "\n",
      "Dataset After Imputation:\n",
      "     Stores     Product  Sales\n",
      "0    costco  Dishwasher  310.0\n",
      "1    costco   Table-Fan  250.0\n",
      "2  best_buy  Dishwasher  390.0\n",
      "3  best_buy   Table-Fan  390.0\n",
      "4      maxi  Dishwasher  170.0\n",
      "5      maxi   Table-Fan  170.0\n",
      "6   walmart  Dishwasher  230.0\n",
      "7   walmart   Table-Fan  175.0\n"
     ]
    }
   ],
   "source": [
    "#Using the median\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Stores': ['Costco', 'Costco', 'Best_Buy', 'Best_Buy', 'Maxi', 'Maxi', 'Walmart', 'Walmart'],\n",
    "    'Product': ['Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan', 'Dishwasher', 'Table-Fan'],\n",
    "    'Sales': [310, 250, np.nan, 390, 170, np.nan, 230, 175]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "df['Stores'] = df['Stores'].str.lower()\n",
    "\n",
    "group_median = df.groupby('Stores')['Sales'].transform('median')\n",
    "\n",
    "print(\"\\nMean Sales per Store:\")\n",
    "print(group_median)\n",
    "\n",
    "\n",
    "df['Sales'] = df['Sales'].fillna(group_median)\n",
    "\n",
    "print(\"\\nDataset After Imputation:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b72bbd-8f73-4e5e-b08f-64f99d8c5beb",
   "metadata": {},
   "source": [
    "### Imputation using K-Nearest Neighbor\n",
    "\n",
    "KNN imputation replaces missing values in a dataset by considering the values of the closest data points. A distance metric, such as the Euclidean distance can be used to determine the similarity between data points. The missing value will then be replaced by the average (or weighted average) of the nearest neighbors corresponding values. Closer neighbors are given more importance as they are considered more similar to the data point with the missing value. Also keeping in mind that, KNN imputation is sensitive to the choice of k which is the number of neighbors and the distance metric used, which can affect the imputation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13beb4a8-3887-4573-a279-808887695fb2",
   "metadata": {},
   "source": [
    "K-NN is a supervised machine learning algorithm that can be used for both classification and regression tasks.With respect to classification problems the class of the data point will be determined based on the majority vote among its k-nearest neigbors and for the regression problems the value of the missing data point will be determined based on the average of the values of its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04a814-6755-41da-9367-28b6c0ab44af",
   "metadata": {},
   "source": [
    "KNN classification example:\n",
    "Let us classify a new data point (5,2) into any one of the classes x1 or x2:\n",
    "\n",
    "class x1: (2,3) (4,5) (7,8) \n",
    "class x2: (1,4) (2,8) (5,8) \n",
    "​\n",
    "Let us choose k value that is the nearest neighbors value to be 3.\n",
    "Let us calculate the distance to find thw 2 nearest neigbors from (5,2)\n",
    "\n",
    "The distance metric is calculated using the Euclidean distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c012ef19-77ab-4fc0-bb07-960f98c1c47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point: (5, 2)\n",
      "k Nearest Neighbors: [((2, 3), 'x1', 3.1622776601683795), ((4, 5), 'x1', 3.1622776601683795), ((1, 4), 'x2', 4.47213595499958)]\n",
      "Predicted Class: x1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "x1 = [(2, 3), (4, 5), (7, 8)]\n",
    "x2 = [(1, 4), (2, 8), (5, 8)]\n",
    "data_point = (5, 2)\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "distances = []\n",
    "for point in x1:\n",
    "    distances.append((point, 'x1', euclidean_distance(data_point, point)))\n",
    "for point in x2:\n",
    "    distances.append((point, 'x2', euclidean_distance(data_point, point)))\n",
    "\n",
    "distances.sort(key=lambda x: x[2])\n",
    "\n",
    "k = 3\n",
    "k_nearest_neighbors = distances[:k]\n",
    "\n",
    "classes = [neighbor[1] for neighbor in k_nearest_neighbors]\n",
    "predicted_class = Counter(classes).most_common(1)[0][0]\n",
    "\n",
    "print(\"Data point:\", data_point)\n",
    "print(\"k Nearest Neighbors:\", k_nearest_neighbors)\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c94ff294-4d17-45c2-9dc6-da4c1df3a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Imputation from the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc1561e8-de51-4144-acad-207c75bd92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[2.5 1.6 4.8]\n",
      " [6.5 nan 2.3]\n",
      " [3.7 5.  2.2]\n",
      " [6.  7.5 nan]]\n",
      "The imputed data where missing values are replaced:\n",
      "[[2.5        1.6        4.8       ]\n",
      " [6.5        6.99620365 2.3       ]\n",
      " [3.7        5.         2.2       ]\n",
      " [6.         7.5        2.28277099]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data = [\n",
    "    [2.5, 1.6, 4.8],\n",
    "    [6.5, np.nan, 2.3],\n",
    "    [3.7, 5.0, 2.2],\n",
    "    [6.0, 7.5, np.nan]\n",
    "]\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"distance\")\n",
    "\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(np.array(data))\n",
    "print(\"The imputed data where missing values are replaced:\")\n",
    "print(imputed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96331293-d790-42e5-99e9-0f59f7238afa",
   "metadata": {},
   "source": [
    "When weights is set to weights='distance' we use this so that closer neighbors will have more importance and similarity while considering them for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89b6bf-ad76-44d2-bf69-460b401e3260",
   "metadata": {},
   "source": [
    "### Imputation by a partial mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc251344-de11-4e77-9b68-f8abe764afa3",
   "metadata": {},
   "source": [
    "With repect to partial mean here the missing values will be replaced by calculating the mean of the non-missing values in that particular column respectively and then that value will be used to replace the missing value only in that column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a22e5bd-30fa-4e6e-8177-c3fe01b287fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed Dataset:\n",
      "   Feature 1  Feature 2  Feature 3\n",
      "0        2.7        3.9        1.1\n",
      "1        4.3        5.4        2.3\n",
      "2        5.5        5.1        3.8\n",
      "3        6.4        7.2        2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrea\\AppData\\Local\\Temp\\ipykernel_10896\\3608757844.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    \"Feature 1\": [2.7, 4.3, 5.5, 6.4],\n",
    "    \"Feature 2\": [3.9, np.nan, 5.1, 7.2],\n",
    "    \"Feature 3\": [1.1, 2.3, 3.8, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column].fillna(df[column].mean(), inplace=True)\n",
    "    \n",
    "print(\"Imputed Dataset:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70568392-05ff-470f-a2a2-c39aa82ff505",
   "metadata": {},
   "source": [
    "### Imputation by singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9371715-1454-42e2-8df4-3c5bdc3db4ee",
   "metadata": {},
   "source": [
    "SVD that is singular value decomposition cane be used for reducing the dimensionality of thw dataset, as it helps in finding the most important or crucial features or patterns in a dataset. SVD will help in keeping only the most important patterns and thereby it will reconstruct the original data as a low-rank matrix, and this version will capture only the most important information without any other unnecessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff8b07-cb21-440b-9b3c-4209e1113122",
   "metadata": {},
   "source": [
    "Let us look through the steps with respect to singular value decomposition:\n",
    " Our data is represented inthe form of a matrix and the columns represent our features or attributes and our rows represent our data value, SVD will break our matrix into three parts that is Σ: which represent the singular values, larger singular values represent the most important patterns and then there is U: which captures how the data points are related and then comes V^T which shows how the features are related so A=UΣV^T. Now only the top k singular values and vectors will be used to reconstruct the matrix. So the reconstructed matrix will help us in replacing the missing values in the original matix from the predicted values that we get through the reconstructed matrix, and this reconstructed matrix consists of the main core information about our data.\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db7d1b41-5441-4ab7-8ffd-0d740f541da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data which consists of the missing values:\n",
      "[[3.5 nan 2.8]\n",
      " [2.6 4.9 nan]\n",
      " [nan 5.5 6.7]\n",
      " [3.3 nan 1.4]]\n",
      "\n",
      "The data after the mean imputation is performed:\n",
      "[[3.5        5.2        2.8       ]\n",
      " [2.6        4.9        3.63333333]\n",
      " [3.13333333 5.5        6.7       ]\n",
      " [3.3        5.2        1.4       ]]\n",
      "\n",
      "Reconstructed Data having rank = 2:\n",
      "[[3.3118157  5.32654906 2.78172426]\n",
      " [2.8341647  4.74253031 3.65607451]\n",
      " [3.0749034  5.53929261 6.69432551]\n",
      " [3.34166687 5.17198011 1.40404653]]\n",
      "\n",
      "Final data after imputation:\n",
      "[[3.5        5.32654906 2.8       ]\n",
      " [2.6        4.9        3.65607451]\n",
      " [3.0749034  5.5        6.7       ]\n",
      " [3.3        5.17198011 1.4       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "data = np.array([\n",
    "    [3.5, np.nan, 2.8],\n",
    "    [2.6, 4.9, np.nan],\n",
    "    [np.nan, 5.5, 6.7],\n",
    "    [3.3, np.nan, 1.4]\n",
    "])\n",
    "\n",
    "def replace_nan_with_mean(data):\n",
    "    column_means = np.nanmean(data, axis=0)\n",
    "    data_replaced_with_mean = np.where(np.isnan(data), column_means, data)\n",
    "    return data_replaced_with_mean\n",
    "\n",
    "data_replaced_with_mean = replace_nan_with_mean(data)\n",
    "\n",
    "U, Sigma, VT = svd(data_replaced_with_mean, full_matrices=False)\n",
    "\n",
    "rank = 2\n",
    "Sigma_reduced = np.diag(Sigma[:rank])  \n",
    "U_reduced = U[:, :rank]\n",
    "VT_reduced = VT[:rank, :]\n",
    "\n",
    "data_reconstructed = np.dot(U_reduced, np.dot(Sigma_reduced, VT_reduced))\n",
    "\n",
    "imputed_data = np.where(np.isnan(data), data_reconstructed, data)\n",
    "\n",
    "print(\"Original data which consists of the missing values:\")\n",
    "print(data)\n",
    "print(\"\\nThe data after the mean imputation is performed:\")\n",
    "print(data_replaced_with_mean)\n",
    "print(\"\\nReconstructed Data having rank = 2:\")\n",
    "print(data_reconstructed)\n",
    "print(\"\\nFinal data after imputation:\")\n",
    "print(imputed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e3844-d22a-4b6a-ace1-e1588ed41898",
   "metadata": {},
   "source": [
    "### Using the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410fade-74f7-4eaa-a4dd-282f3f5ba814",
   "metadata": {},
   "source": [
    "With respect to EM algorithm there are two steps:\n",
    "1.The Expectation step (E) and \n",
    "2.Maximization step (M) \n",
    "\n",
    "In the Expectation step the missing values are estimated based on the observed data and the current parameters of the model.\n",
    "Keeping the relationships, the correlation between variables, the predictions of missing values can be made, followed by the Maximization step where the parameters of the model need to be updated in order to maximize the likelihood of the data, including the predicted values for the missing values.\n",
    "Then these two steps should be performed iteratively until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee700892-b35b-43ea-9978-b939456566ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
